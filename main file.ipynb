{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hjjbB1pLerV"
      },
      "source": [
        "\n",
        "**Authors:** Zofia Walczewska"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3VvETXYXApL",
        "outputId": "c0628e5d-a5ba-4e54-f20f-723347bb4d2d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ]
        }
      ],
      "source": [
        "#Setup / Imports\n",
        "import pandas as pd\n",
        "import matplotlib as plt\n",
        "import numpy as np\n",
        "import math\n",
        "import statsmodels.api as sm\n",
        "# Train / Test split:\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "yMGxvngEXJ_l",
        "outputId": "c5795644-3f45-4df8-8eea-e9b9575434db"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-ca34d64b-d668-49e1-a1ac-891bc5e0ba4f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>price</th>\n",
              "      <th>bedrooms</th>\n",
              "      <th>bathrooms</th>\n",
              "      <th>sqft_living</th>\n",
              "      <th>sqft_lot</th>\n",
              "      <th>floors</th>\n",
              "      <th>waterfront</th>\n",
              "      <th>view</th>\n",
              "      <th>condition</th>\n",
              "      <th>grade</th>\n",
              "      <th>sqft_above</th>\n",
              "      <th>sqft_basement</th>\n",
              "      <th>yr_built</th>\n",
              "      <th>yr_renovated</th>\n",
              "      <th>zipcode</th>\n",
              "      <th>lat</th>\n",
              "      <th>long</th>\n",
              "      <th>sqft_living15</th>\n",
              "      <th>sqft_lot15</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6681</th>\n",
              "      <td>3.500</td>\n",
              "      <td>3</td>\n",
              "      <td>2.25</td>\n",
              "      <td>1860</td>\n",
              "      <td>8378</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>1860</td>\n",
              "      <td>0</td>\n",
              "      <td>1995</td>\n",
              "      <td>0.0</td>\n",
              "      <td>98038</td>\n",
              "      <td>47.3875</td>\n",
              "      <td>-122.032</td>\n",
              "      <td>1870</td>\n",
              "      <td>8378</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17798</th>\n",
              "      <td>5.925</td>\n",
              "      <td>4</td>\n",
              "      <td>3.00</td>\n",
              "      <td>2170</td>\n",
              "      <td>8240</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>1370</td>\n",
              "      <td>800</td>\n",
              "      <td>1968</td>\n",
              "      <td>0.0</td>\n",
              "      <td>98052</td>\n",
              "      <td>47.6291</td>\n",
              "      <td>-122.093</td>\n",
              "      <td>2020</td>\n",
              "      <td>7944</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18854</th>\n",
              "      <td>2.555</td>\n",
              "      <td>2</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1440</td>\n",
              "      <td>43560</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>1150</td>\n",
              "      <td>290</td>\n",
              "      <td>1965</td>\n",
              "      <td>0.0</td>\n",
              "      <td>98027</td>\n",
              "      <td>47.4916</td>\n",
              "      <td>-122.082</td>\n",
              "      <td>1870</td>\n",
              "      <td>56628</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13478</th>\n",
              "      <td>13.300</td>\n",
              "      <td>4</td>\n",
              "      <td>2.25</td>\n",
              "      <td>3260</td>\n",
              "      <td>4640</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5</td>\n",
              "      <td>9</td>\n",
              "      <td>2360</td>\n",
              "      <td>900</td>\n",
              "      <td>1907</td>\n",
              "      <td>0.0</td>\n",
              "      <td>98112</td>\n",
              "      <td>47.6272</td>\n",
              "      <td>-122.312</td>\n",
              "      <td>3240</td>\n",
              "      <td>5800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10509</th>\n",
              "      <td>3.891</td>\n",
              "      <td>2</td>\n",
              "      <td>1.00</td>\n",
              "      <td>840</td>\n",
              "      <td>5400</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>840</td>\n",
              "      <td>0</td>\n",
              "      <td>1948</td>\n",
              "      <td>0.0</td>\n",
              "      <td>98118</td>\n",
              "      <td>47.5489</td>\n",
              "      <td>-122.271</td>\n",
              "      <td>1340</td>\n",
              "      <td>5400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16125</th>\n",
              "      <td>2.900</td>\n",
              "      <td>2</td>\n",
              "      <td>1.00</td>\n",
              "      <td>930</td>\n",
              "      <td>7740</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>930</td>\n",
              "      <td>0</td>\n",
              "      <td>1924</td>\n",
              "      <td>0.0</td>\n",
              "      <td>98125</td>\n",
              "      <td>47.7091</td>\n",
              "      <td>-122.292</td>\n",
              "      <td>1250</td>\n",
              "      <td>7740</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19004</th>\n",
              "      <td>3.150</td>\n",
              "      <td>3</td>\n",
              "      <td>2.50</td>\n",
              "      <td>1730</td>\n",
              "      <td>6368</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>1730</td>\n",
              "      <td>0</td>\n",
              "      <td>1993</td>\n",
              "      <td>0.0</td>\n",
              "      <td>98038</td>\n",
              "      <td>47.3505</td>\n",
              "      <td>-122.032</td>\n",
              "      <td>1780</td>\n",
              "      <td>6597</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9094</th>\n",
              "      <td>6.850</td>\n",
              "      <td>3</td>\n",
              "      <td>2.50</td>\n",
              "      <td>3450</td>\n",
              "      <td>8000</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>2970</td>\n",
              "      <td>480</td>\n",
              "      <td>1927</td>\n",
              "      <td>1975.0</td>\n",
              "      <td>98116</td>\n",
              "      <td>47.5605</td>\n",
              "      <td>-122.402</td>\n",
              "      <td>1880</td>\n",
              "      <td>6135</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3537</th>\n",
              "      <td>3.260</td>\n",
              "      <td>6</td>\n",
              "      <td>1.50</td>\n",
              "      <td>1930</td>\n",
              "      <td>8400</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>1030</td>\n",
              "      <td>900</td>\n",
              "      <td>1971</td>\n",
              "      <td>0.0</td>\n",
              "      <td>98146</td>\n",
              "      <td>47.4869</td>\n",
              "      <td>-122.340</td>\n",
              "      <td>1780</td>\n",
              "      <td>9520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10054</th>\n",
              "      <td>3.150</td>\n",
              "      <td>2</td>\n",
              "      <td>2.25</td>\n",
              "      <td>1290</td>\n",
              "      <td>2436</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "      <td>1290</td>\n",
              "      <td>0</td>\n",
              "      <td>1984</td>\n",
              "      <td>0.0</td>\n",
              "      <td>98052</td>\n",
              "      <td>47.6803</td>\n",
              "      <td>-122.156</td>\n",
              "      <td>1360</td>\n",
              "      <td>3088</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>13397 rows × 19 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ca34d64b-d668-49e1-a1ac-891bc5e0ba4f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ca34d64b-d668-49e1-a1ac-891bc5e0ba4f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ca34d64b-d668-49e1-a1ac-891bc5e0ba4f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "        price  bedrooms  bathrooms  sqft_living  sqft_lot  floors  waterfront  \\\n",
              "6681    3.500         3       2.25         1860      8378     2.0         0.0   \n",
              "17798   5.925         4       3.00         2170      8240     1.0         0.0   \n",
              "18854   2.555         2       1.00         1440     43560     1.0         0.0   \n",
              "13478  13.300         4       2.25         3260      4640     2.0         0.0   \n",
              "10509   3.891         2       1.00          840      5400     1.0         0.0   \n",
              "...       ...       ...        ...          ...       ...     ...         ...   \n",
              "16125   2.900         2       1.00          930      7740     1.0         0.0   \n",
              "19004   3.150         3       2.50         1730      6368     2.0         0.0   \n",
              "9094    6.850         3       2.50         3450      8000     3.0         0.0   \n",
              "3537    3.260         6       1.50         1930      8400     1.0         0.0   \n",
              "10054   3.150         2       2.25         1290      2436     2.0         0.0   \n",
              "\n",
              "       view  condition  grade  sqft_above  sqft_basement  yr_built  \\\n",
              "6681    0.0          3      7        1860              0      1995   \n",
              "17798   0.0          4      8        1370            800      1968   \n",
              "18854   0.0          4      7        1150            290      1965   \n",
              "13478   0.0          5      9        2360            900      1907   \n",
              "10509   0.0          4      7         840              0      1948   \n",
              "...     ...        ...    ...         ...            ...       ...   \n",
              "16125   0.0          3      6         930              0      1924   \n",
              "19004   0.0          3      7        1730              0      1993   \n",
              "9094    0.0          4      8        2970            480      1927   \n",
              "3537    0.0          3      7        1030            900      1971   \n",
              "10054   0.0          3      7        1290              0      1984   \n",
              "\n",
              "       yr_renovated  zipcode      lat     long  sqft_living15  sqft_lot15  \n",
              "6681            0.0    98038  47.3875 -122.032           1870        8378  \n",
              "17798           0.0    98052  47.6291 -122.093           2020        7944  \n",
              "18854           0.0    98027  47.4916 -122.082           1870       56628  \n",
              "13478           0.0    98112  47.6272 -122.312           3240        5800  \n",
              "10509           0.0    98118  47.5489 -122.271           1340        5400  \n",
              "...             ...      ...      ...      ...            ...         ...  \n",
              "16125           0.0    98125  47.7091 -122.292           1250        7740  \n",
              "19004           0.0    98038  47.3505 -122.032           1780        6597  \n",
              "9094         1975.0    98116  47.5605 -122.402           1880        6135  \n",
              "3537            0.0    98146  47.4869 -122.340           1780        9520  \n",
              "10054           0.0    98052  47.6803 -122.156           1360        3088  \n",
              "\n",
              "[13397 rows x 19 columns]"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "houses_dataset = pd.read_csv(\"/content/houses (1).csv\", index_col=0)\n",
        "houses_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYUXPL4crqth",
        "outputId": "974f1bba-486d-420f-d5ed-fc370e34a0e4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(13397, 19)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "houses_dataset.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2jJxMX7XeMi"
      },
      "source": [
        "1. A quick statistical study about the dataset : predictive varibales, target variable, link between the variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Qmh5njfu1Fin"
      },
      "outputs": [],
      "source": [
        "def my_regression(data, idx_p, idx_t):\n",
        "    X = data.iloc[:,idx_p]\n",
        "    X = sm.add_constant(X) \n",
        "    Y = data.iloc[:,idx_t]\n",
        "    model = sm.OLS(Y, X).fit() \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHI_ehjbXgFL",
        "outputId": "5dbf0806-f663-4fa9-cf15-259ba0800548"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bedrooms\n",
            "R^2 0.0918534076592672\n",
            "I_t 169063.80737826164\n",
            "I_m 15529.086819543292\n",
            "I_r 153534.72055871834\n",
            "critical probability const        1.305121e-31\n",
            "bedrooms    1.276790e-282\n",
            "dtype: float64\n",
            "\n",
            "best var 1\n",
            "bathrooms\n",
            "R^2 0.27690175512124326\n",
            "I_t 134614.54727550482\n",
            "I_m 37275.00440543886\n",
            "I_r 97339.54287006595\n",
            "critical probability const        0.678856\n",
            "bathrooms    0.000000\n",
            "dtype: float64\n",
            "\n",
            "best var 2\n",
            "sqft_living\n",
            "R^2 0.49640102806105757\n",
            "I_t 93751.78006044979\n",
            "I_m 46538.48000456143\n",
            "I_r 47213.30005588836\n",
            "critical probability const          6.023324e-22\n",
            "sqft_living    0.000000e+00\n",
            "dtype: float64\n",
            "\n",
            "best var 3\n",
            "sqft_lot\n",
            "R^2 0.007529065473369179\n",
            "I_t 184761.92755494983\n",
            "I_m 1391.0846495471103\n",
            "I_r 183370.8429054027\n",
            "critical probability const       0.000000e+00\n",
            "sqft_lot    8.191470e-24\n",
            "dtype: float64\n",
            "\n",
            "best var 3\n",
            "floors\n",
            "R^2 0.06673028102799827\n",
            "I_t 173740.81820156818\n",
            "I_m 11593.773624625002\n",
            "I_r 162147.04457694318\n",
            "critical probability const     1.885723e-190\n",
            "floors    3.539163e-203\n",
            "dtype: float64\n",
            "\n",
            "best var 3\n",
            "waterfront\n",
            "R^2 0.07359512492671638\n",
            "I_t 172462.83438666185\n",
            "I_m 12692.423841901977\n",
            "I_r 159770.41054475988\n",
            "critical probability const          0.000000e+00\n",
            "waterfront    1.130316e-224\n",
            "dtype: float64\n",
            "\n",
            "best var 3\n",
            "view\n",
            "R^2 0.1506986535664976\n",
            "I_t 158108.96660355214\n",
            "I_m 23826.80838394564\n",
            "I_r 134282.1582196065\n",
            "critical probability const    0.0\n",
            "view     0.0\n",
            "dtype: float64\n",
            "\n",
            "best var 3\n",
            "condition\n",
            "R^2 0.001081402898655437\n",
            "I_t 185962.2474072333\n",
            "I_m 201.1001133866616\n",
            "I_r 185761.14729384665\n",
            "critical probability const        1.409304e-165\n",
            "condition     1.407046e-04\n",
            "dtype: float64\n",
            "\n",
            "best var 3\n",
            "grade\n",
            "R^2 0.43650499676537324\n",
            "I_t 104902.2387893594\n",
            "I_m 45790.35140342973\n",
            "I_r 59111.887385929665\n",
            "critical probability const    0.0\n",
            "grade    0.0\n",
            "dtype: float64\n",
            "\n",
            "best var 3\n",
            "sqft_above\n",
            "R^2 0.3747164369013217\n",
            "I_t 116405.0173838501\n",
            "I_m 43618.873351512724\n",
            "I_r 72786.14403233738\n",
            "critical probability const         1.354813e-14\n",
            "sqft_above    0.000000e+00\n",
            "dtype: float64\n",
            "\n",
            "best var 3\n",
            "sqft_basement\n",
            "R^2 0.0983914249789678\n",
            "I_t 167846.66676451475\n",
            "I_m 16514.67272093056\n",
            "I_r 151331.99404358419\n",
            "critical probability const             0.000000e+00\n",
            "sqft_basement    1.188407e-303\n",
            "dtype: float64\n",
            "\n",
            "best var 3\n",
            "yr_built\n",
            "R^2 0.0018226988537450861\n",
            "I_t 185824.24510934646\n",
            "I_m 338.7016385588517\n",
            "I_r 185485.5434707876\n",
            "critical probability const       1.468354e-02\n",
            "yr_built    7.680309e-07\n",
            "dtype: float64\n",
            "\n",
            "best var 3\n",
            "yr_renovated\n",
            "R^2 0.014062110932065885\n",
            "I_t 183545.7125205727\n",
            "I_m 2581.0401705693675\n",
            "I_r 180964.67235000333\n",
            "critical probability const           0.000000e+00\n",
            "yr_renovated    3.711907e-43\n",
            "dtype: float64\n",
            "\n",
            "best var 3\n",
            "zipcode\n",
            "R^2 0.0018621646109022727\n",
            "I_t 185816.8980232888\n",
            "I_m 346.0216516066049\n",
            "I_r 185470.8763716822\n",
            "critical probability const      3.618221e-07\n",
            "zipcode    5.834977e-07\n",
            "dtype: float64\n",
            "\n",
            "best var 3\n",
            "lat\n",
            "R^2 0.09531485746468382\n",
            "I_t 168419.4115416324\n",
            "I_m 16052.872205376618\n",
            "I_r 152366.53933625578\n",
            "critical probability const    3.718156e-286\n",
            "lat      9.779089e-294\n",
            "dtype: float64\n",
            "\n",
            "best var 3\n",
            "long\n",
            "R^2 0.00023060811069708365\n",
            "I_t 186120.63439823608\n",
            "I_m 42.920927860319864\n",
            "I_r 186077.71347037578\n",
            "critical probability const    0.051048\n",
            "long     0.078812\n",
            "dtype: float64\n",
            "\n",
            "best var 3\n",
            "sqft_living15\n",
            "R^2 0.3360368161846665\n",
            "I_t 123605.75347806347\n",
            "I_m 41536.08386087522\n",
            "I_r 82069.66961718825\n",
            "critical probability const            1.277368e-28\n",
            "sqft_living15    0.000000e+00\n",
            "dtype: float64\n",
            "\n",
            "best var 3\n",
            "sqft_lot15\n",
            "R^2 0.005970611084734623\n",
            "I_t 185052.05498018087\n",
            "I_m 1104.8738507175888\n",
            "I_r 183947.18112946328\n",
            "critical probability const         0.000000e+00\n",
            "sqft_lot15    3.361396e-19\n",
            "dtype: float64\n",
            "\n",
            "best var 3\n",
            "best var based on statictics: yr_renovated\n",
            "variables that are significant in the presence of the best variable: [ 1.  2.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17. 18.]\n"
          ]
        }
      ],
      "source": [
        "#target variable- Y\n",
        "Y = houses_dataset['price']\n",
        "columns = houses_dataset.columns\n",
        "max_R_2 = 0\n",
        "min_I_r = 1000000000000000000000000000000000000\n",
        "for i in range(1, 19):\n",
        "  model = my_regression(houses_dataset, [i], 0) # fit the model to predict Y using X\n",
        "  print(columns[i])\n",
        "  print(\"R^2\",model.rsquared)\n",
        "  R_2 = model.rsquared\n",
        "  I_t=sum(model.resid**2)\n",
        "  print(\"I_t\", I_t)\n",
        "  I_m=model.rsquared*I_t\n",
        "  print('I_m',I_m)\n",
        "  I_r=I_t-I_m\n",
        "  print('I_r', I_r)\n",
        "  print('critical probability', model.pvalues)\n",
        "  critic_prob = model.pvalues\n",
        "  print()\n",
        "  if R_2 > max_R_2 and I_r < min_I_r:\n",
        "    min_r_2 = R_2\n",
        "    min_I_r = I_r\n",
        "    min_criti_prob = critic_prob\n",
        "    best_variable = i\n",
        "  print('best var', best_variable)\n",
        "print('best var based on statictics:', columns[13])\n",
        "\n",
        "significant_vars = np.array([])\n",
        "#check how the best variable is behaving with other variables\n",
        "for i in range(1, 19):\n",
        "  if i != best_variable:\n",
        "    model = my_regression(houses_dataset, [best_variable, i], 0)\n",
        "    if(model.pvalues[1] < 0.05):\n",
        "      significant_vars = np.append(significant_vars, i)\n",
        "\n",
        "print('variables that are significant in the presence of the best variable:', significant_vars)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4uPhuoQzXgh-"
      },
      "source": [
        "2. Simple regression models : you can use only one predictive variable from the 18. You are asked to choose the best one and evaluate its generalization error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_0VDuN6XjBT",
        "outputId": "6b6d8059-581c-4be1-cb14-7737b1e6c810"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(10047, 19)\n",
            "(3350, 19)\n",
            "bedrooms 12.823332355833903\n",
            "bathrooms 10.208452025657175\n",
            "sqft_living 7.017563087396969\n",
            "sqft_lot 14.037867664522041\n",
            "floors 13.271413353320012\n",
            "waterfront 13.024297177462978\n",
            "view 12.491076142353661\n",
            "condition 14.179990129729239\n",
            "grade 8.052645393015863\n",
            "sqft_above 8.658522831401445\n",
            "sqft_basement 12.914233273027044\n",
            "yr_built 14.131748555142527\n",
            "yr_renovated 14.03828772373208\n",
            "zipcode 14.125603818244285\n",
            "lat 12.844807958972153\n",
            "long 14.168979434868318\n",
            "sqft_living15 9.505683089667501\n",
            "sqft_lot15 14.055965688816476\n",
            "\n",
            "smallest gen error has:  sqft_living\n",
            "the error for simple regression:  7.017563087396969\n"
          ]
        }
      ],
      "source": [
        "model = my_regression(houses_dataset, [13], 0)\n",
        "model.summary()\n",
        "\n",
        "train , test = train_test_split(houses_dataset, test_size = 0.25) \n",
        "print(train.shape)\n",
        "print(test.shape)\n",
        "\n",
        "def my_prediction(my_model, data):\n",
        "    X_new = data[my_model.model.exog_names[1:]]\n",
        "    X_new = sm.add_constant(X_new)\n",
        "    predictions = my_model.predict(X_new)\n",
        "    return predictions\n",
        "\n",
        "def generalization_error_split_1(train, test, idx_p, idx_t):\n",
        "  model = my_regression(train, idx_p, idx_t)\n",
        "  predictions = my_prediction(model, test)\n",
        "  MSE=np.mean((predictions-test.iloc[:,idx_t])**2)\n",
        "  return MSE\n",
        "\n",
        "#generalization_error_split(train, test, [13], 0)\n",
        "min_gen_error_simple_regression = 384441531451994100000000\n",
        "for var in range(1,19):\n",
        "  error = generalization_error_split_1(train, test, [var], 0)\n",
        "  print(columns[var],generalization_error_split_1(train, test, [var], 0))\n",
        "  if error < min_gen_error_simple_regression:\n",
        "    best_var = columns[var]\n",
        "    min_gen_error_simple_regression = error\n",
        "print()\n",
        "print('smallest gen error has: ', best_var)\n",
        "print('the error for simple regression: ', min_gen_error_simple_regression)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jk81qO5QXjWf"
      },
      "source": [
        "3. Multiple regression : use the 18 variables to predict the sale price. Estimate the generalization error of this model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4TIbi44jXli0",
        "outputId": "58c2b4aa-b2c9-4e1f-c1a9-b502c5b6ac6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "error of prediction using all 18 variables: 4.270437542475231\n"
          ]
        }
      ],
      "source": [
        "error_multiple_regression_18_vars = generalization_error_split_1(train, test, [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, \n",
        "                                                      11, 12, 13, 14, 15, 16, 17, 18], 0)\n",
        "\n",
        "print('error of prediction using all 18 variables:', error_multiple_regression_18_vars)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfWWD_CvXl1E"
      },
      "source": [
        "4. Variable selection : apply the different variable selection techniques seen during the course to select interesting models. Can you find better models than before ? (according to the estimation of the generalization error)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZonv5X6XoUn",
        "outputId": "b3805a2a-057a-4beb-f1a2-80328b736679"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the array of best prediction variables: [ 3 15  7  9 12  6 14 16  2 17  8  5]\n",
            "error for thiose chosen variables: 3.4994216578150064\n"
          ]
        }
      ],
      "source": [
        "def my_prediction(my_model, data):\n",
        "  X_new = data[my_model.model.exog_names[1:]]\n",
        "  X_new = sm.add_constant(X_new)\n",
        "  predictions = my_model.predict(X_new)\n",
        "  return predictions\n",
        "\n",
        "def generalization_error_split(model, test, idx_t):\n",
        "  predictions = my_prediction(model, test)\n",
        "  MSE=np.mean((predictions-test.iloc[:,idx_t])**2)\n",
        "  return MSE \n",
        "\n",
        "def step_selection_gen_error(train, test, v_s, v_nu, idx_t):\n",
        "  best_error = 1000000000000\n",
        "  best_v_s = []\n",
        "  for var in v_nu:\n",
        "    new_v_s = np.append(v_s,[var])\n",
        "    model2=my_regression(train, new_v_s, idx_t)\n",
        "    error = generalization_error_split(model2, test, idx_t)\n",
        "    if(error < best_error):\n",
        "      best_error = error\n",
        "      best_v_s = var\n",
        "  return best_v_s, best_error, model\n",
        "     \n",
        "def forward_selection_gen_error(data, idx_p, idx_t):\n",
        "  train, test = train_test_split(data, test_size=0.25, random_state=20)\n",
        "  v_s = []\n",
        "  best_error = 10000000000\n",
        "  v_nu = np.array(idx_p)\n",
        "  stop = False\n",
        "  while stop == False:\n",
        "    result = step_selection_gen_error( train, test, v_s, v_nu, idx_t )\n",
        "    var = int(result[0])\n",
        "    error = result[1]\n",
        "    if (result[1] < best_error):\n",
        "      v_s = np.append( v_s , [ var ] ).astype(int)\n",
        "      best_error = error\n",
        "      v_nu = np.delete( v_nu, np.where(v_nu == var) )\n",
        "    else:\n",
        "      stop = True\n",
        "  return v_s, best_error\n",
        "    \n",
        "best_vars = forward_selection_gen_error(train, [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, \n",
        "                                                12, 13, 14, 15, 16, 17, 18], 0)\n",
        "print('the array of best prediction variables:', best_vars[0])\n",
        "error_multiple_regression_best_choice = best_vars[1]\n",
        "print('error for thiose chosen variables:', error_multiple_regression_best_choice)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGGmKn23Xov_"
      },
      "source": [
        "5. Non-linear models : add some non-linear variables (that you can couple with variable selection techniques) to try to design more accurate models. First - strict stopping criteria."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7dIGbb5Wm8c"
      },
      "source": [
        "The model with all the variables and their power 2:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXbDLbjvXrDH",
        "outputId": "cd47ebe8-383e-4bb4-d7fe-02b75000322a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generalization error for second degree polynomials for every variable: 3.3708521209557523\n"
          ]
        }
      ],
      "source": [
        "def add_polynomial_feature(data, idx_p, power):\n",
        "  new_data = data.copy(deep = True)\n",
        "  for i in range(0, len(idx_p)):\n",
        "      for j in power:\n",
        "          for k in range(2, j+1):\n",
        "              new_data['{}_pow_{}'.format(new_data.columns[idx_p[i]],k)] = new_data.iloc[:,idx_p[i]]**k\n",
        "  return(new_data)\n",
        "\n",
        "train_poly_all_var_2_pow = train\n",
        "test_poly_all_var_2_pow = test\n",
        "for i in range(1, 19):\n",
        "    train_poly_all_var_2_pow = add_polynomial_feature(train_poly_all_var_2_pow, [i], [2])\n",
        "    test_poly_all_var_2_pow = add_polynomial_feature(test_poly_all_var_2_pow, [i], [2])\n",
        "\n",
        "model=my_regression(train_poly_all_var_2_pow,[*range(1, 37)],0)\n",
        "Y_pred = my_prediction(model, test_poly_all_var_2_pow)\n",
        "error_poly_all_variables = generalization_error_split(model, test_poly_all_var_2_pow, 0)\n",
        "print('generalization error for second degree polynomials for every variable:', error_poly_all_variables)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSF30AypWunT"
      },
      "source": [
        "The model with 9 polynomials for each variable:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtvkaMkCVv_M",
        "outputId": "3361fefb-992c-48b3-f89f-30261aa6f142"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "generalization errors for predictions based on 9 polynomials for each variable:\n",
            "bedrooms\n",
            "12.729314852944812\n",
            "bathrooms\n",
            "9.239283997629402\n",
            "sqft_living\n",
            "7.536283058415072\n",
            "sqft_lot\n",
            "31.508485699724638\n",
            "floors\n",
            "13.042625004906043\n",
            "waterfront\n",
            "13.02429717746298\n",
            "view\n",
            "12.425719437856076\n",
            "condition\n",
            "14.073576342838045\n",
            "grade\n",
            "7.277724871663395\n",
            "sqft_above\n",
            "9.614725170591603\n",
            "sqft_basement\n",
            "28.46334027914823\n",
            "yr_built\n",
            "13.80728230433764\n",
            "yr_renovated\n",
            "41.299505267735405\n",
            "zipcode\n",
            "13.734210931526887\n",
            "lat\n",
            "11.642130734302112\n",
            "long\n",
            "14.1226878729471\n",
            "sqft_living15\n",
            "9.603850057551691\n",
            "sqft_lot15\n",
            "22.894910479820705\n",
            "\n",
            "the smallest error: grade\n",
            "7.277724871663395\n"
          ]
        }
      ],
      "source": [
        "train_poly_2 = train\n",
        "test_poly_2 = test\n",
        "min = 351134689174875540000\n",
        "print()\n",
        "print('generalization errors for predictions based on 9 polynomials for each variable:')\n",
        "for i in range(1, 19):\n",
        "  for j in range(2, 10):\n",
        "    train_poly2 = add_polynomial_feature(train_poly_2, [i], [j])\n",
        "    test_poly2 = add_polynomial_feature(test_poly_2, [i], [j])\n",
        "  model2=my_regression(train_poly2,[i,19, 20, 21, 22, 23, 24, 25, 26],0)\n",
        "  Y_pred_2 = my_prediction(model2, test_poly2)\n",
        "  error2 = generalization_error_split(model2, test_poly2, 0)\n",
        "  if (error2 < min):\n",
        "    min = error2\n",
        "    min_column = columns[i]\n",
        "    min_column_nb = i\n",
        "  print(columns[i])\n",
        "  print(error2)\n",
        "\n",
        "print()\n",
        "print('the smallest error:', min_column)\n",
        "print(min)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4WIhgAXZRid"
      },
      "source": [
        "The model with the best amount of polynomials for the chosen variable that had the smallest error with the polynomial 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b67cdmNSV3v3",
        "outputId": "ea03c7e4-2aaa-41db-f18b-58f1625493cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best min error for this variable:  7.276393148195483\n",
            "the v_s that won:  [ 9 19 20 21 22 23 24 25]\n"
          ]
        }
      ],
      "source": [
        "### checking the best amount of polynomials for the best variable chosen before \n",
        "for j in range(3, 10):\n",
        "    train_poly = add_polynomial_feature(train, [min_column_nb], [j])\n",
        "    test_poly = add_polynomial_feature(test, [min_column_nb], [j])\n",
        "\n",
        "v_s = [min_column_nb,19]\n",
        "X = train_poly.iloc[:,min_column_nb] \n",
        "X = sm.add_constant(X) \n",
        "Y = train_poly['price']\n",
        "model2=my_regression(train_poly,v_s,0)\n",
        "error_poly_one_variable = generalization_error_split(model2, test_poly, 0)\n",
        "best_v_s = v_s\n",
        "\n",
        "for i in range(20, 27):\n",
        "  v_s = np.append(v_s,[i])\n",
        "  X = train_poly.iloc[:,min_column_nb] \n",
        "  X = sm.add_constant(X) \n",
        "  Y = train_poly['price']\n",
        "  model2=my_regression(train_poly,v_s,0)\n",
        "\n",
        "  if (generalization_error_split(model2, test_poly, 0) < min):\n",
        "    error_poly_one_variable = generalization_error_split(model2, test_poly, 0)\n",
        "    best_v_s = v_s\n",
        "\n",
        "print('best min error for this variable: ', error_poly_one_variable)\n",
        "print('the v_s that won: ', best_v_s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZ_exvSNzvYc",
        "outputId": "c98a08ac-ee67-4fa0-9865-9f3b3d886a09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "error for polynomial regression with best variable power 2:  7.276393148195483\n",
            "error for polynomial regression with all variables power 2:  3.3708521209557523\n",
            "error for multiple regression best choice:  3.4994216578150064\n",
            "error for multiple regression all variables:  4.270437542475231\n",
            "error for simple regression and best variable:  7.017563087396969\n"
          ]
        }
      ],
      "source": [
        "print('error for polynomial regression with best variable power 2: ', error_poly_one_variable)\n",
        "print('error for polynomial regression with all variables power 2: ', error_poly_all_variables)\n",
        "print('error for multiple regression best choice: ', error_multiple_regression_best_choice)\n",
        "print('error for multiple regression all variables: ', error_multiple_regression_18_vars)\n",
        "print('error for simple regression and best variable: ', min_gen_error_simple_regression)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Hk95YCMaiLV"
      },
      "source": [
        "Generation of predtictions for the competition dataset based on the model that had the smallest error out of all the models tested above:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "wAybh2Qt09JS"
      },
      "outputs": [],
      "source": [
        "competition = pd.read_csv(\"/content/houses_competition.csv\", index_col=0)\n",
        "\n",
        "for i in range(2, 19):\n",
        "    houses_dataset = add_polynomial_feature(houses_dataset, [i], [2])\n",
        "    competition = add_polynomial_feature(competition, [i-1], [2])\n",
        "\n",
        "model=my_regression(houses_dataset,[*range(1, 36)],0)\n",
        "\n",
        "# If you have one regression model named ’my_model’\n",
        "pred = my_prediction(model, competition)\n",
        "\n",
        "pred = pd.DataFrame({'ID': pred.index, 'Price':pred})\n",
        "pred.to_csv('my_submission.csv', index=False)\n",
        "# This will create a csv file ’my_submission.csv’ with the predictions\n",
        "# of the competition dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ILIQSUTX1EY"
      },
      "source": [
        "6. Non-linear model with the second stopping criteria. Stop the forward selection until the 3rd unsuccesfull iteration (meaning that the error isnt getting better).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vT89RIbfX90U",
        "outputId": "a3c43df5-6289-45fb-b8c2-ee1a0ed8f857"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the array of best prediction variables with 3rd iteration stopping criteria: (array([ 3, 15,  7,  9, 12,  6, 14, 16,  2, 17,  8]), 3.4994216578150064, <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7fbbf817f490>)\n",
            "3.4994216578150064\n"
          ]
        }
      ],
      "source": [
        "def forward_selection_gen_error_succes_iteration(data, idx_p, idx_t, max_nb_of_iter):\n",
        "  train, test = train_test_split(data, test_size=0.25, random_state=20)\n",
        "  v_s = []\n",
        "  best_error = 10000000000\n",
        "  v_nu = np.array(idx_p)\n",
        "  stop = False\n",
        "  nb_of_iterations = 0\n",
        "  while stop == False and v_nu != []:\n",
        "    result = step_selection_gen_error( train, test, v_s, v_nu, idx_t )\n",
        "    var = int(result[0])\n",
        "    error = result[1]\n",
        "    if (result[1] < best_error):\n",
        "      best_error = error\n",
        "      best_v_s = v_s\n",
        "      model = result[2]\n",
        "    else:\n",
        "      #count the number of iterations that havent found a better error\n",
        "      nb_of_iterations = nb_of_iterations +1\n",
        "\n",
        "    v_s = np.append( v_s , [ var ] ).astype(int)\n",
        "    v_nu = np.delete( v_nu, np.where(v_nu == var) )\n",
        "\n",
        "    #if the i-th unsuccesfull iteration isn't better, stop the search.\n",
        "    if( nb_of_iterations == max_nb_of_iter):\n",
        "      stop = True\n",
        "\n",
        "  return best_v_s, best_error, model\n",
        "\n",
        "best_vars = forward_selection_gen_error_succes_iteration(train, [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, \n",
        "                                                12, 13, 14, 15, 16, 17, 18], 0, 5)\n",
        "print('the array of best prediction variables with 3rd iteration stopping criteria:', best_vars)\n",
        "error_multiple_regression_with_stopping_criteria_1 = best_vars[1]\n",
        "print(error_multiple_regression_with_stopping_criteria_1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKp5Yxqo27mG"
      },
      "source": [
        "Using forward selection with iteration stopping criteria for finding the best variables from all the variables and their polynomials of 2nd degree:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0Lzg8wg26dL",
        "outputId": "8e0def16-9a71-45aa-bad6-71f0f1bf9b98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "vars chosen from the array with all 2nd polynomials chosen by forward selection with 3rd non-success iteration\n",
            "[27 12  3 15  6 33  7 14 34 35  2  8 30 16 13  4 32 22 18 24 23]\n",
            "error for this model 3.1639397777785296\n"
          ]
        }
      ],
      "source": [
        "#generating 2nd polynomials of all variables existing in the table\n",
        "train_poly_all_var_2_pow = train\n",
        "test_poly_all_var_2_pow = test\n",
        "for i in range(1, 20):\n",
        "    train_poly_all_var_2_pow = add_polynomial_feature(train_poly_all_var_2_pow, [i], [2])\n",
        "    test_poly_all_var_2_pow = add_polynomial_feature(test_poly_all_var_2_pow, [i], [2])\n",
        "\n",
        "best_vars_2nd_polys_iter_criteria = forward_selection_gen_error_succes_iteration(train_poly_all_var_2_pow, [*range(1, 37)], 0, 5)\n",
        "print()\n",
        "print('vars chosen from the array with all 2nd polynomials chosen by forward selection with 3rd non-success iteration')\n",
        "print(best_vars_2nd_polys_iter_criteria[0])\n",
        "print('error for this model', best_vars_2nd_polys_iter_criteria[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tV9i74XAX-LE"
      },
      "source": [
        "7. Non-linear model with the third stopping criteria- exhaustive method. Check all the possible variations and chose the best one. Not suitable for dataset with a lot of variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTS68nDJYEHi",
        "outputId": "385e8465-823d-4fa4-88f7-f70b569d1957"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the array of best prediction variables with exhastive stopping criteria: (array([ 3, 15,  7,  9, 12,  6, 14, 16,  2, 17,  8]), 3.4994216578150064, <statsmodels.regression.linear_model.RegressionResultsWrapper object at 0x7fbbf817f490>)\n",
            "3.4994216578150064\n"
          ]
        }
      ],
      "source": [
        "def forward_selection_gen_error_exhaustive(data, idx_p, idx_t):\n",
        "  train, test = train_test_split(data, test_size=0.25, random_state=20)\n",
        "  v_s = []\n",
        "  best_error = 10000000000\n",
        "  v_nu = np.array(idx_p)\n",
        "  stop = False\n",
        "  nb_of_iterations = 0\n",
        "  #check the error while v_nu has still values -> exhaustive method, trying \n",
        "  #every possibility and chosing the one with the smallest error\n",
        "  while v_nu.size > 0 :\n",
        "    result = step_selection_gen_error( train, test, v_s, v_nu, idx_t )\n",
        "    var = int(result[0])\n",
        "    error = result[1]\n",
        "    if (result[1] < best_error):\n",
        "      best_error = error\n",
        "      best_v_s = v_s\n",
        "      model = result[2]\n",
        "\n",
        "    v_s = np.append( v_s , [ var ] ).astype(int)\n",
        "    v_nu = np.delete( v_nu, np.where(v_nu == var) )\n",
        "\n",
        "  return best_v_s, best_error, model\n",
        "\n",
        "best_vars = forward_selection_gen_error_exhaustive(train, [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, \n",
        "                                                12, 13, 14, 15, 16, 17, 18], 0)\n",
        "print('the array of best prediction variables with exhastive stopping criteria:', best_vars)\n",
        "error_multiple_regression_with_stopping_criteria_2 = best_vars[1]\n",
        "print(error_multiple_regression_with_stopping_criteria_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDH4CbRl3ZFn"
      },
      "source": [
        "Using exhaustive forward selection to choose the best variables among all the variables and their 2nd degree polynomials:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-WbV0OY3j61",
        "outputId": "4e1ccf6d-27ad-4774-f6af-a9ffd719d182"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "vars chosen from the array with all 2nd polynomials chosen by exhaustive forward selection\n",
            "[27 12  3 15  6 33  7 14 34 35  2  8 30 16 13  4 32 22 18 24 23]\n",
            "error for this model 3.1639397777785296\n"
          ]
        }
      ],
      "source": [
        "best_vars_2nd_polys_exhauxtive = forward_selection_gen_error_exhaustive(train_poly_all_var_2_pow, [*range(1, 37)], 0)\n",
        "print()\n",
        "print('vars chosen from the array with all 2nd polynomials chosen by exhaustive forward selection')\n",
        "print(best_vars_2nd_polys_exhauxtive[0])\n",
        "print('error for this model', best_vars_2nd_polys_exhauxtive[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtkjpqtgYEZh"
      },
      "source": [
        "8. Use the critical probability as a performance criteria for variable selection (for this performance\n",
        "criteria, only one particular stopping criterion can be used : stop when all selected variables are significant and no one from the other variables are significant when added). Significant means with a critical probability less than 0.05."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Y-fAv6UYL3X",
        "outputId": "ed0a2303-3aec-4ded-926a-2236d8136933"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the array of best prediction variables: [ 3 15  7  9 12  6 14 16  2]\n",
            "error for thiose chosen variables: 3.539976938346405\n"
          ]
        }
      ],
      "source": [
        "def step_selection_gen_error_critical_probability(train, test, v_s, v_nu, idx_t):\n",
        "  best_error = 1000000000000\n",
        "  best_v_s = []\n",
        "  for var in v_nu:\n",
        "    new_v_s = np.append(v_s,[var])\n",
        "    model2=my_regression(train, new_v_s, idx_t)\n",
        "    error = generalization_error_split(model2, test, idx_t)\n",
        "    significance_of_addeded_var = model2.pvalues[-1]\n",
        "    if(error < best_error):\n",
        "      best_error = error\n",
        "      best_v_s = var\n",
        "  return best_v_s, best_error, significance_of_addeded_var\n",
        "     \n",
        "def forward_selection_gen_error_critical_probability(data, idx_p, idx_t):\n",
        "  train, test = train_test_split(data, test_size=0.25, random_state=20)\n",
        "  v_s = []\n",
        "  best_error = 10000000000000000000000000000000\n",
        "  v_nu = np.array(idx_p)\n",
        "  stop = False\n",
        "  while stop == False:\n",
        "    result = step_selection_gen_error_critical_probability( train, test, v_s, v_nu, idx_t )\n",
        "    var = int(result[0])\n",
        "    error = result[1]\n",
        "    significance = result[2]\n",
        "    if (result[1] < best_error and significance < 0.05):\n",
        "      v_s = np.append( v_s , [ var ] ).astype(int)\n",
        "      best_error = error\n",
        "      v_nu = np.delete( v_nu, np.where(v_nu == var) )\n",
        "    else:\n",
        "      stop = True\n",
        "  return v_s, best_error\n",
        "    \n",
        "best_vars = forward_selection_gen_error_critical_probability(train, [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, \n",
        "                                                12, 13, 14, 15, 16, 17, 18], 0)\n",
        "print('the array of best prediction variables:', best_vars[0])\n",
        "error_forward_selection_with_critical_probability = best_vars[1]\n",
        "print('error for thiose chosen variables:', error_forward_selection_with_critical_probability)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITUTL4RLYMUe"
      },
      "source": [
        "9.  Implement the backward selection procedure with exhautive stopping criteria."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZy9YsmCYQws",
        "outputId": "49572638-cf83-4c3d-b276-1f004515a7e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the array of best prediction variables chosen in backward selection: [ 1  2  3  6  7  8  9 10 11 12 13 14 15 16 18]\n",
            "error of the backward selection 4.633464308049005\n"
          ]
        }
      ],
      "source": [
        "def backward_step_selection_gen_error(train, test, v_s, idx_t):\n",
        "  best_error = 1000000000000000000000000000000000\n",
        "  for var in v_s:\n",
        "    new_v_s = np.delete( v_s, np.where(v_s == var) )\n",
        "    model2=my_regression(train, new_v_s, idx_t)\n",
        "    error = generalization_error_split(model2, test, idx_t)\n",
        "    if(error < best_error):\n",
        "      best_error = error\n",
        "      best_var = var\n",
        "  return best_var, best_error, model2\n",
        "\n",
        "def backward_selection_gen_error(data, idx_p, idx_t):\n",
        "  train, test = train_test_split(data, test_size=0.25, random_state=20)\n",
        "  v_s = np.array(idx_p)\n",
        "  model2=my_regression(train, v_s, idx_t)\n",
        "  best_error = generalization_error_split(model2, test, idx_t)\n",
        "\n",
        "  #check the error while v_nu has still values -> exhaustive method, trying \n",
        "  #every possibility and chosing the one with the smallest error\n",
        "  while v_s.size > 0 :\n",
        "    result = backward_step_selection_gen_error( train, test, v_s, idx_t )\n",
        "    var = int(result[0])\n",
        "    error = result[1]\n",
        "    if (result[1] < best_error):\n",
        "      best_error = error\n",
        "      best_v_s = np.delete( v_s, np.where(v_s == var))\n",
        "      model = result[2]\n",
        "    v_s = np.delete( v_s, np.where(v_s == var))\n",
        "  return best_v_s, best_error, model\n",
        "\n",
        "train, test = train_test_split(houses_dataset, test_size=0.25, random_state=20)    \n",
        "best_vars = backward_selection_gen_error(train, [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, \n",
        "                                                12, 13, 14, 15, 16, 17, 18], 0)\n",
        "print('the array of best prediction variables chosen in backward selection:', best_vars[0])\n",
        "error_backward_selection = best_vars[1]\n",
        "print('error of the backward selection', error_backward_selection)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbNdmRDG4xr7"
      },
      "source": [
        "Using backward selection to find the best variables among all the variables and theis 2nd degree polynomials:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5lsg0F274x9F",
        "outputId": "75b49737-8dbb-40f2-f8f3-7b3a1d4078b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "vars chosen from the array with all 2nd polynomials chosen by exhaustive forward selection\n",
            "[ 2  3  4  5  6  7  8 12 13 14 15 16 18 22 23 24 27 30 32 33 34 35]\n",
            "error for this model 3.163939679836004\n"
          ]
        }
      ],
      "source": [
        "best_vars_2nd_polys_backward_selection = backward_selection_gen_error(train_poly_all_var_2_pow, [*range(1, 37)], 0)\n",
        "print()\n",
        "print('vars chosen from the array with all 2nd polynomials chosen by exhaustive forward selection')\n",
        "print(best_vars_2nd_polys_backward_selection[0])\n",
        "print('error for this model', best_vars_2nd_polys_backward_selection[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtlyhUq05B4p"
      },
      "source": [
        "Errors from the models with different stopping criteria and backward selection used on the set of all the variables and their 2nd degree polynomials:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3BRDIv_4-XZ",
        "outputId": "5a5e8102-0f35-4356-c37e-7d16d53cfd3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "backward selection 3.163939679836004\n",
            "exhaustive forward selection 3.1639397777785296\n",
            "forward selection with 5 iterations criteria 3.1639397777785296\n"
          ]
        }
      ],
      "source": [
        "print('backward selection', best_vars_2nd_polys_backward_selection[1])\n",
        "print('exhaustive forward selection',  best_vars_2nd_polys_exhauxtive[1])\n",
        "print('forward selection with 5 iterations criteria', best_vars_2nd_polys_iter_criteria[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKg7Elc-5yHJ"
      },
      "source": [
        "Backward selection seems to have the best results- for finding the best variable among all the variables and 2nd polynomials. \n",
        "\n",
        "Lets try to generate the predictions of prices for the competition dataset based on this model that choses the best variables among all the variables and theis 2nd degree polynomials:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KnM-RRs35vGY",
        "outputId": "6f084d0f-b5f2-40e9-f689-1d47e414bd7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "15529     3.129591\n",
            "3233      4.055427\n",
            "14381     5.350966\n",
            "3201     12.147862\n",
            "3425      2.054009\n",
            "           ...    \n",
            "7989      5.073475\n",
            "9973      3.057983\n",
            "9007      3.378706\n",
            "16345    14.711418\n",
            "13582     1.719877\n",
            "Length: 2365, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "result = backward_selection_gen_error(houses_dataset, [*range(1, 36)], 0)\n",
        "model = result[2]\n",
        "\n",
        "# If you have one regression model named ’my_model’\n",
        "pred = my_prediction(model, competition)\n",
        "print(pred)\n",
        "pred = pd.DataFrame({'ID': pred.index, 'Price':pred})\n",
        "pred.to_csv('my_submission.csv', index=False)\n",
        "# This will create a csv file ’my_submission.csv’ with the predictions\n",
        "# of the competition dataset"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Untitled1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
